---
layout: default
title: Practice Resources (Kaggle)
---
Why Should You Use Kaggle?  
Kaggle is rightfully considered one of the best resources for learning NLP in practice. In this section, we will explain why Kaggle is so useful.

There is an excellent article on this topic that might be helpful to read:  
<a href="https://skillbox.ru/media/code/kaggle_dlya_nachinayushchego_data_sayentista_sorevnovatsya_nelzya_uchitsya/?ysclid=lvaazetks3682760695">Kaggle for a Beginner Data Scientist: Competing is Not the Goal, Learning Is</a>

- Kaggle provides ready-made datasets and pre-set tasks, so the user only needs to start solving them. Sounds easy, right? You can find a wide variety of tasks on Kaggle, and it‚Äôs very convenient that past competitions are still available, allowing you to practice and discover new solutions for them.

- Kaggle is very friendly to beginners. For many tasks, there are detailed introductions that help you get acquainted with new tools. There‚Äôs a discussion section where you can find answers to your questions or get hints if you‚Äôre stuck. You can filter to search only for competitions for beginners. For example, at the moment, someone with little experience can participate in the following competitions:
1. <a href="https://www.kaggle.com/competitions/nlp-getting-started">Natural Language Processing with Disaster Tweets</a>  
Participants are tasked with classifying tweets related to emergencies as either relevant to real disasters or not. The data includes tweet text with labels indicating their relation to real catastrophes. This competition is perfect for beginners in NLP and machine learning
2. <a href="https://www.kaggle.com/competitions/contradictory-my-dear-watson">Contradictory, My Dear Watson</a>  
The task is to solve the problem of detecting textual contradictions in a multilingual corpus. Participants need to predict whether two statements are neutral, contradictory, or imply one another. This competition involves working with multilingual models and transformers such as BERT.
3. <a href="https://www.kaggle.com/competitions/gemma-language-tuning">Google - Unlock Global Communication with Gemma</a>  
Participants compete in building models that effectively adapt to language processing tasks considering domain-specific contexts. This includes fine-tuning models for specialized texts and requirements. A great choice for those looking to dive deeper into the challenges of language model adaptation.

- The most important thing to remember is: we are here not to win competitions (although it would be great if you manage to), but to learn, to become better than we were yesterday. Kaggle (and success in competitions) is not a goal in itself, but a tool for mastering NLP.

- Here you can look at the best tasks from previous years. Their level is generally higher.  
<a href="/extra_material/kaggle_competitions/">Kaggle Competition Reviews</a>

- Additionally, Kaggle should not be criticized as only a resource for practice, as it also offers excellent theoretical guides, which are perfect for starting your NLP journey from scratch:  
1. Your very first NLP guide:  
<a href="https://www.kaggle.com/competitions/nlp-getting-started">Getting started with NLP for absolute beginners</a>
2. Diving into deep learning:  
<a href="https://www.kaggle.com/code/tanulsingh077/deep-learning-for-nlp-zero-to-transformers-bert">Deep Learning For NLP: Zero To Transformers & BERT</a>
3. Another comprehensive guide for slightly higher-level NLP:  
<a href="https://www.kaggle.com/code/andreshg/nlp-glove-bert-tf-idf-lstm-explained">NLP üìù GloVe, BERT, TF-IDF, LSTM... üìù Explained</a>
4. A guide to preprocessing text before using embeddings: 
<a href="https://www.kaggle.com/code/christofhenkel/how-to-preprocessing-when-using-embeddings">How to: Preprocessing when using embeddings</a>  

- There is also a great video that explains the logic behind any Kaggle project, which will help you get started with any applied task:  
<a href="https://www.youtube.com/watch?v=Jn8c3oe_GWU">Kaggle Live-Coding: Scoping & Starting an NLP Project | Kaggle</a>
