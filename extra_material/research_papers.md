---
layout: default
title: Научные статьи
---
В этом разделе мы приведем несколько простых для понимания, легко читающихся статей из области nlp, которые хорошо подойдут для тех, кто только начинает знакомство с этой замечательной областью.

1. <a href="https://web.stanford.edu/~jurafsky/slp3/ed3book.pdf">Speech and Language Processing</a>  
Вводная работа для знакомства с компьютерной лингвистикой и распознаванием речи.
2. <a href="https://www.jair.org/index.php/jair/article/view/11030">Quora Question Pairs</a>  
Статья про использование глубокого обучения в NLP.
3. <a href="https://arxiv.org/abs/1706.03762">Attention Is All You Need</a>  
Работа, вводящая в научный контекст концепт трансформера, очень важный для современного NLP.
4.<a href="https://arxiv.org/abs/1301.3781">Efficient Estimation of Word Representations in Vector Space</a>  
Работа, вводящая не менее важный концепт эмбеддинга.
5. <a href="https://arxiv.org/abs/1607.01759">Bag of Tricks for Efficient Text Classification</a>  
Работа, вводящая концепты bag-of-tricks и bag-of-words, а также показывающая эффективность простого по своим принципам классификатора fastText.
6. <a href="https://arxiv.org/abs/1703.03130">A Structured Self-attentive Sentence Embedding</a>  
Эта работа развивает идею эмбеддингов, делая их еще более эффективными.
7. <a href="https://arxiv.org/abs/2005.14165">Language Models are Few-Shot Learners</a>  
Работа, описывающая тренировку GPT-3.
8.<a href="https://arxiv.org/abs/2001.04451">Reformer: The Efficient Transformer</a>  
Данная статья посвящена поиску формулы эффективного трансформера. Сейчас трансформеры становятся столь громоздкими, что их идее требуется какое-то переосмысление. Авторы предлагают концепт Реформера.
9. <a href="https://arxiv.org/abs/1611.04558">Google's Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation</a>  
Статья, представляющая Neural Machine Translation, модель, которая одна может осуществлять переводы между многими языками сразу.
10. <a href="https://arxiv.org/abs/2004.05150">Longformer: The Long-Document Transformer</a>  
Статья, предлагающая новый вид трансформеров - лонгформер, который отличается тем, что при его использование задействование памяти устройства растет не квадратично, а линейно.
11. <a href="https://arxiv.org/abs/2001.09977">Towards a Human-like Open-Domain Chatbot</a>  
Работа, описывающая тренировку чат-бота с открытым доменом на материале огромного объема. Предполагается, что данный чат-бот сможет порождать в диалоге ответы намного более близкие к человеческим, чем сейчас удается другим моделям.
12.<a href="https://arxiv.org/abs/2003.10555">ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators</a>  
Эта статья предлагает новый способ обучения языковых моделей, основанный не на порождении возможных вариантов, а на исключении невозможных, что позволяет модели учиться на куда большем материале.
